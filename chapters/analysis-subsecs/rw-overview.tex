\section{Reweighting overview}
\label{sec:rw-overview}

This technique derives a mapping to get an event-by-event weight from a lower tagged region to a higher tagged region (like a generalized ABCD method).

The lower \Pqb-tagged region consists of events with exactly two \Pqb-tags (which will subsequently be referred to as ``2\Pqb'' events), and we reweight to match the distributions of events with four or more \Pqb-tags (``4\Pqb'' events).  For ``2\Pqb'' events, the leading two non-\Pqb tagged jets are taken for the other HC jets, and these four jets are still paired into HCs with the $\text{min} \Delta R_{jj}^{HC 1}$ pairing algorithm and pass through the rest of the analysis selection.

The reweighting maps are derived in CR 1 (as shown in \Fig{\ref{fig:massplanes-allYrs-data}}) since the events in the 4b SR are not observed until the analysis selection is finalized. 
The reweighting consists of deriving the maps $w(x)$

\begin{equation}
	p_{4b}(x) = w(x) \cdot p_{2b}(x),
\end{equation}
\noindent
where $x$ is a set of features characterizing the kinematics of the event.

In the partial Run~2 analysis, this reweighting function $w(x)$  was found as a sequential set of corrections \cite{EXOT-2016-30}. These correction factors were cubic splines fit to 4\Pqb / 2\Pqb ratios of the 1d histograms which defined the QCD background model (taken as data - $t\bar{t}$). 
Although this approach had worked reasonably well with the limited dataset available at the beginning of Run~2 (27 \ifb), the sequential reweighting was not guaranteed to account for the correlations, and it was limited by the curse of dimensionality, as $x$ was just a 6d vector.

An alternative approach is to use the ``Density Ratio Trick'' where a classifier $D(x)$ can be used to discriminate between the 2\Pqb vs the 4\Pqb data. 
If we label 4\Pqb events as class 1 and 2\Pqb events as class 0, then the classifier score $z(x)$ can be passed through the non-linear sigmoid function $g(z) = \frac{1}{1 + \exp(-z)}$  to ensure the classifier outputs are also between 0 and 1.
In a probabilistic interpretation of the classifier output as $p_{4b} (x) = D(x)$ and $p_{2b}(x) = 1- D(x)$ then the parameters of $D(x)$ can be found by maximum likelihood update steps of the logistic loss \cite{cs229-notes1}.

Given this association between the classifier output and the probabilities, we can also see that there is a bijective map between the classifier output and the reweighting maps: 
\begin{equation*}
w(x) = \frac{p_{4b}(x)}{p_{2b}(x)} = \frac{D(x)}{1 - D(x)} \quad \implies \quad D(x) = \frac{1}{1+w(x)}
\end{equation*}
By using classifiers to implement the reweighting, we can avoid the curse of dimensionality by using more modern ML classifiers which scale to higher dimensional inputs, such as BDTs or NNs.
This innovation was used in the 4\Pqb SUSY analysis where a BDT classifier was used to implement an inclusive background reweighting \cite{SUSY-2017-02}.

For this analysis, instead of using a NN directly as a classifier, we instead learn $Q(x)= \log w(x)$, as proposed in \cite{1911.00405}.
We use a learn NN to learn this $Q(x)$ by minimizing the loss function: 
\begin{equation}
	\mathcal{L}[Q] = \mathbb{E}_{x\sim p_{2b}} \left[ \exp\left(  \frac{1}{2} Q(x) \right) \right] + \mathbb{E}_{x\sim p_{4b}} \left[ \exp\left(  - \frac{1}{2} Q(x) , \right) \right]
\end{equation}
\noindent
which results in a model learning $Q^*(x) = \log w^*(x)$ (proof in \App{\ref{rw-loss-fct}}).

This neural solution still lets us avoid the curse of dimensionality and take into account correlations, but we no longer need to have a non-linearity on the output classifier score as $Q(x) \in \mathbb{R}$.
This lets us avoid the saturating gradients problem inherent with using sigmoids in deep learning solutions.
Also, learning the $\log$ of the likelihood ratio, naturally enforces positive weights as $w(x) = e^{Q(x)}$.
Experiments in \cite{1911.00405} show this exponential loss gives (slightly) better performance than the logistic loss -- and our team saw the same thing for our analysis optimization as well. 

These mappings are derived in a kinematically similar control region (shown as CR 1 in \Fig{\ref{fig:ggF-massplanes-allYrs-dat-4b-preXwt}}). Then an error on this method from this choice of training region is taken by using an alternative control region (CR 2) and using the difference between the CR1 and CR2 predictions as an error bar.
Applying these reweighting maps in the SR gives us a background estimate of the observed data in a blinded SR.

In practice, there is some amount of noisiness due to the initializations of the NNs. To this extent, the NN for each background estimate is retrained 100 times and the average of the weights is taken as the nominal weight.
Additionally, for each of the NN trainings the CR1 training events have a weight sampled from a Poisson distribution with a mean of one to account for the finite statistics of the training dataset.

The ggF (VBF) networks have 3 hidden layers with 50 (20) hidden units each and use ReLU non-linearities. The training is done using stochastic gradient descent with the adam optimizer with a learning rate of 0.001, and a batch size of 1024 events. 20\% of the CR1 training events is held out as a validation set, and the optimization steps continue until the loss on the validation set hasn't improved in the last 15 epochs\footnote{An epoch is a single pass over the training dataset}.

The input variables for the ggF and VBF reweightings are delineated in \Tab{\ref{tab:rw-inputs}}. While the sequential reweighting from \cite{EXOT-2016-30} was limited to $x \in \mathbb{R}^6$, by our ML solution avoiding the curse of dimensionality, $x \in \mathbb{R}^12$ for ggF and $x\in \mathbb{R}^9$ for VBF. I 
Since we use different triggers between each of the years, the ggF background estimate has a different background estimate derived for each of the three years. Since VBF has two orders of magnitude less statistics than ggF, it was not as sensitive to this effect, so instead for VBF the training was done inclusively for the years with the year passed as an extra variable.

For ggF, the background estimates were derived before the \Xwt cut, while for the VBF the trainings were derived after the \Xwt cut.\footnote{We saw for ggF that we had the same performance whether we trained before or after the \Xwt cut.}
 
\def\checkmark{\tikz\fill[scale=0.4](0,.35) -- (.25,0) -- (1,.7) -- (.25,.15) -- cycle;}
\renewcommand{\arraystretch}{1.2}

\begin{table}[htbp]
	\centering
	\begin{tabular}{p{10cm} | p{2cm} | p{2cm}  }
	{Variable description} & {\bfseries ggF} & {\bfseries VBF} \\
	\hline\hline
	$\log(\Delta R_1)$: between the closest two HC jets & \checkmark  & \\
	$\log(\Delta R_2)$ between the other two HC jets &\checkmark  & \\
	$\log(p_T)$ of the 4th leading HC jet & \checkmark  & \\
	$\log(p_T)$ of the 2nd leading HC jet & \checkmark  & \\
	$\left< | HC \eta  | \right >$:  average absolute value of the HC jets $\eta$ & \checkmark  & \checkmark  \\
	$\log(p_{T,HH})$& \checkmark  & \\
	$\Delta R_{HH}$  & \checkmark  & \\
	$\Delta \phi$ between the jets in the leading HC & \checkmark  & \\
	$\Delta \phi$ between the jets in the subleading HC & \checkmark  & \\
	$\log(X_{\PW\Pqt})$ & \checkmark & \checkmark  \\
	Number of jets in the event &  \checkmark  & \\
	Trigger bucket index  & \checkmark  & \checkmark  \\
	Year index	& & \checkmark  \\
	Second smallest $\Delta R$ between the jets in the leading HC (out of the three possible pairings) 	& & \checkmark  \\
	Maximum di-jet mass out of the possible pairings of  HC jets 	& & \checkmark  \\
	Minimum di-jet mass out of the possible pairings of HC jets 	& & \checkmark \\
	Energy of the leading HC	& & \checkmark \\
	Energy of the subleading HC	& & \checkmark  \\
	\hline
	\end{tabular}
	\caption{Set of input variables used for the 2\Pqb to 4\Pqb reweighting for the ggF and VBF channels. The variables included in the background estimate are denoted with a checkmark.}
	\label{tab:rw-inputs}
\end{table}

% Back to default table spacing
\renewcommand{\arraystretch}{1}



