\chapter{Reweighting loss function}
\label{rw-loss-fct}

The derivation below follows the proof from \cite{sean-thesis}.

Remember in \Ch{\ref{sec:rw-overview}} we introduced the loss function:

\begin{equation}
	\mathcal{L}[Q] = \mathbb{E}_{x\sim p_{2b}} \left[ \exp\left(  \frac{1}{2} Q(x) \right) \right] + \mathbb{E}_{x\sim p_{4b}} \left[ \exp\left(  - \frac{1}{2} Q(x) \right) \right]
\end{equation}

We want to show that $Q^*(x) = \log w^*(x) $ minimizes this loss for $w^*(x) = p_{4b}(x) / p_{2b}(x) $.
Since $\log$ is a (monotonically increasing) bijective function,
we can just show that the loss function with respect to $w$ is minimized when $w^*(x) = p_{4b}(x) / p_{2b}(x)$. Substituting $Q(x) = \log w(x)$ into the loss function and writing out the integrals defining these expectation values, we have: 

\begin{align}
	\mathcal{L}[w] &= \mathbb{E}_{x\sim p_{2b}} \left[ \sqrt{ w(x)} \right] + \mathbb{E}_{x\sim p_{4b}} \left[ \frac{1}{\sqrt{ w(x)}} \right] \\
	&= \int \left[ \sqrt{w(x)} p_{2b}(x) +\frac{1}{ \sqrt{w(x)} } p_{4b}(x) \right] dx.
\end{align}
\noindent
Calculus of variations can be used to solve for the function that is the extrema of this integral. Let

\begin{equation*}
\mathcal{I}( x_1 , \ldots , x_n, w, w' ) =  \sqrt{w(x)} p_{2b}(x) +\frac{1}{ \sqrt{w(x)} } p_{4b}(x).
\end{equation*}
\noindent
The solution to the extrema of the loss is given by the Euler-Lagrange equation:

\begin{equation*}
\frac{\partial \mathcal{I}}{\partial w} - \frac{d}{dx_i}  \frac{\partial \mathcal{I}}{\partial w'} = 0
\end{equation*}

\noindent
Since there is no explicit dependence on the $x_i$, we just need to solve for $\frac{\partial \mathcal{I}}{\partial w} =0$.

\begin{align*}
\frac{\partial \mathcal{I}}{\partial w} =  \frac{1}{2} w^{-1/2} p_{2b} &- \frac{1}{2} w^{-3/2} p_{4b} = 0 \\
\implies w^*(x) &= p_{4b} / p_{2b}
\end{align*}

This is the $w^*$ that we were searching for! To show that it's also a minimum, we take the $2^{nd}$ functional derivative:

\begin{equation*}
\frac{\partial^2 \mathcal{I}}{\partial w^2} = - \frac{1}{4} w ^{-3/2} p_{2b} + \frac{3}{4} w^{-5/2} p_{4b} = \frac{1}{4} w ^{-5/2} \left( 3 p_{4b} - \frac{1}{4} w p_{2b} \right)
\end{equation*}

\noindent
and evaluate it at the extrema point:

\begin{equation*}
 \frac{\partial^2 \mathcal{I}}{\partial w^2} \biggr\rvert_{w = p_{4b} / p_{2b}} =  \frac{1}{4} \left( \frac{p_{4b}}{p_{2b}} \right) ^{-5/2} \left( \frac{11}{4} p_{4b}  \right) > 0
 \end{equation*}

\noindent
because both $p_{4b}(x)$ and $p_{2b}(x)$ are positive. $\mathcal{L}[w]$ is concave up at $w^*(x) = p_{4b} (x) / p_{2b}(x)$ and this $w^*$ is in fact a minimum. 
This justifies our use of stochastic gradient descent with this loss function to derive these reweighting maps.
